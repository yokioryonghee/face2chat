# app.py
# âœ… ì ˆëŒ€ ê²½ë¡œ import ë°©ì‹
import sys
import os

# 'modules' í´ë” ìì²´ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'modules'))

import gradio as gr
print(f"DEBUG: Gradio version in use: {gr.__version__}") # â­ï¸ ì´ ì¤„ ì¶”ê°€ â­ï¸

from modules.pipeline import Face2ChatPipeline
from modules.emotion_detector import EmotionDetector
from modules.speech_to_text import SpeechToText
from modules.chatbot_engine import ChatbotEngine
from modules.text_to_speech import TextToSpeech
from modules.vision_analyzer import VisionAnalyzer # â­ï¸ VisionAnalyzer ì„í¬íŠ¸ â­ï¸

import numpy as np # numpy ì„í¬íŠ¸
import soundfile as sf # soundfile ì„í¬íŠ¸ (ì˜¤ë””ì˜¤ ì €ì¥ìš©)
import tempfile        # ì„ì‹œ íŒŒì¼ ìƒì„±ìš©
import shutil          # ì„ì‹œ íŒŒì¼ ë””ë ‰í† ë¦¬ ê´€ë¦¬ìš© (í˜„ì¬ëŠ” ì§ì ‘ ì‚¬ìš© ì•ˆ í•¨)


# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
detector = EmotionDetector()
stt = SpeechToText()
bot = ChatbotEngine()
tts = TextToSpeech()
vision_analyzer = VisionAnalyzer() # â­ï¸ VisionAnalyzer ì¸ìŠ¤í„´ìŠ¤ ìƒì„± â­ï¸
pipeline = Face2ChatPipeline(detector, stt, bot, tts, vision_analyzer) # â­ï¸ pipelineì— ì „ë‹¬ â­ï¸

# Gradioì—ì„œ í˜¸ì¶œí•  í•¨ìˆ˜
def run_pipeline(image, audio):
    # Gradioê°€ ì œê³µí•˜ëŠ” ì„ì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ STT ìˆ˜í–‰
    # audioëŠ” (sample_rate, numpy_array) íŠœí”Œ í˜•íƒœ ë˜ëŠ” íŒŒì¼ ê²½ë¡œì¼ ìˆ˜ ìˆìŒ
    # í˜„ì¬ speech_to_text.pyëŠ” íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ, íŠœí”Œì´ë¼ë©´ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    audio_input_path = None

    if isinstance(audio, tuple): # audioê°€ (sample_rate, numpy_array) íŠœí”Œë¡œ ë“¤ì–´ì˜¬ ê²½ìš°
        sr, audio_array = audio
        if audio_array is not None and audio_array.size > 0:
            try:
                # ì„ì‹œ íŒŒì¼ ìƒì„± ì‹œ tempfile.mkdtemp() ì‚¬ìš© (gr.make_temp_dir() ê²½ê³  í•´ê²°)
                # Gradioê°€ ì„ì‹œ ë””ë ‰í† ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬í•˜ë¯€ë¡œ, ë³„ë„ì˜ ë””ë ‰í† ë¦¬ë¥¼ ë§Œë“¤ í•„ìš” ì—†ìŒ
                # Gradio 4.0+ ë²„ì „ì—ì„œëŠ” gr.make_temp_dir()ì´ ì—†ì–´ì¡Œìœ¼ë¯€ë¡œ tempfileì„ ì§ì ‘ ì‚¬ìš©.
                # í•˜ì§€ë§Œ Gradio ë‚´ë¶€ì ìœ¼ë¡œ ì„ì‹œíŒŒì¼ì„ ìƒì„±í•˜ê³  ê²½ë¡œë¥¼ ë„˜ê²¨ì¤„ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ,
                # ì—¬ê¸°ì„œëŠ” ë°›ì€ ê²½ë¡œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ìš°ì„ ì‹œí•˜ê³ ,
                # íŠœí”Œ í˜•íƒœì˜ ì˜¤ë””ì˜¤ ì…ë ¥ë§Œ íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                
                # Gradioê°€ ë„˜ê²¨ì£¼ëŠ” ì˜¤ë””ì˜¤ íŠœí”Œì„ WAV íŒŒì¼ë¡œ ì €ì¥
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fp:
                    temp_audio_file = fp.name
                    sf.write(temp_audio_file, audio_array, sr)
                audio_input_path = temp_audio_file
                print(f"ğŸ¶ Gradio íŠœí”Œ ì˜¤ë””ì˜¤ë¥¼ ì„ì‹œ WAV íŒŒì¼ë¡œ ì €ì¥: {audio_input_path}")
            except Exception as e:
                print(f"â— Gradio ì˜¤ë””ì˜¤ íŠœí”Œì„ íŒŒì¼ë¡œ ì €ì¥ ì‹¤íŒ¨: {e}")
                audio_input_path = None # ì˜¤ë¥˜ ì‹œ ìŒì„± ì¸ì‹ ê±´ë„ˆë›°ê¸°
        else:
            print("â— ì˜¤ë””ì˜¤ ì…ë ¥ (íŠœí”Œ)ì´ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            audio_input_path = None
    elif isinstance(audio, str) and os.path.exists(audio): # audioê°€ íŒŒì¼ ê²½ë¡œë¡œ ë“¤ì–´ì˜¬ ê²½ìš°
        audio_input_path = audio
        print(f"ğŸ¶ Gradio íŒŒì¼ ê²½ë¡œ ì˜¤ë””ì˜¤ ì…ë ¥: {audio_input_path}")
    else:
        print("â— ì˜¤ë””ì˜¤ ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        audio_input_path = None

    emotion, text, response, audio_out_tuple = pipeline.run(image, audio_input_path)

    print("ğŸš¨ result from pipeline.run():", (emotion, text, response, "audio_out_tuple_exists")) # print audio_out as string to avoid large console output
    print("ğŸš¨ types:", [type(x) for x in (emotion, text, response, audio_out_tuple)])

    # pipeline.runì—ì„œ ë°˜í™˜ëœ audio_out_tupleì´ (np.ndarray, sample_rate) í˜•ì‹ì¸ì§€ í™•ì¸
    if not (isinstance(audio_out_tuple, tuple) and len(audio_out_tuple) == 2 and
            isinstance(audio_out_tuple[0], np.ndarray) and isinstance(audio_out_tuple[1], (int, float))):
        print("â— pipeline.runì—ì„œ ë°˜í™˜ëœ audio_out í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬´ìŒ ì˜¤ë””ì˜¤ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        sample_rate = 44100
        silence = np.zeros(int(sample_rate * 0.5), dtype=np.float32)
        audio_out_tuple = (silence, sample_rate) # (numpy_array, sample_rate) í˜•ì‹ìœ¼ë¡œ íŠœí”Œ ë°˜í™˜

    # Gradioì— ë°˜í™˜í•˜ê¸° ìœ„í•´ (numpy_array, sample_rate) íŠœí”Œì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    final_audio_output_path = None
    try:
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fp:
            final_audio_output_path = fp.name
            sf.write(final_audio_output_path, audio_out_tuple[0], int(audio_out_tuple[1]))
        print(f"ğŸ¶ ì‘ë‹µ ì˜¤ë””ì˜¤ ì„ì‹œ íŒŒì¼ ì €ì¥: {final_audio_output_path}")
    except Exception as e:
        print(f"â— ì‘ë‹µ ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë¬´ìŒ ì˜¤ë””ì˜¤ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        sample_rate = 44100
        silence = np.zeros(int(sample_rate * 0.5), dtype=np.float32)
        
        try:
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fp:
                final_audio_output_path = fp.name
                sf.write(final_audio_output_path, silence, sample_rate)
            print(f"ğŸ¶ ì˜¤ë¥˜ ëŒ€ì²´ìš© ë¬´ìŒ ì˜¤ë””ì˜¤ ì„ì‹œ íŒŒì¼ ì €ì¥: {final_audio_output_path}")
        except Exception as e_fallback:
            print(f"â— ëŒ€ì²´ ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ë§ˆì € ì‹¤íŒ¨: {e_fallback}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë°˜í™˜ (UIì—ì„œ ì˜¤ë””ì˜¤ ì¬ìƒ ì•ˆë¨)
            final_audio_output_path = None
    
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•œ ê²½ìš°, í•´ë‹¹ íŒŒì¼ ê²½ë¡œë¥¼ Gradioì— ë°˜í™˜.
    # GradioëŠ” ì´ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ UIì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¬ìƒí•©ë‹ˆë‹¤.
    # Gradio 4.0+ ë²„ì „ì€ `type="filepath"`ì¼ ê²½ìš° íŒŒì¼ì„ ìë™ìœ¼ë¡œ ê´€ë¦¬í•˜ë¯€ë¡œ,
    # ëª…ì‹œì ì¸ `os.remove(temp_dir)`ëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    return emotion, text, response, final_audio_output_path


# ì¸í„°í˜ì´ìŠ¤ ì •ì˜
interface = gr.Interface(
    fn=run_pipeline,
    inputs=[
        gr.Image(type="numpy", label="ì–¼êµ´ ì´ë¯¸ì§€ (ì›¹ìº  ì…ë ¥)", streaming=True), # â­ï¸ typeì„ "numpy"ë¡œ ë³€ê²½ â­ï¸
        gr.Audio(type="numpy", label="ìŒì„± ì…ë ¥", streaming=True) # â­ï¸ typeì„ "numpy"ë¡œ ë³€ê²½ â­ï¸
    ],
    outputs=[
        gr.Textbox(label="ê°ì •"),
        gr.Textbox(label="ìŒì„± ì¸ì‹ ê²°ê³¼"),
        gr.Textbox(label="ì±—ë´‡ ì‘ë‹µ"),
        gr.Audio(label="ì‘ë‹µ ìŒì„±", type="filepath", autoplay=True) # â­ï¸ TTS ì¶œë ¥ typeì„ "filepath"ë¡œ ë³€ê²½ â­ï¸
    ],
    live=True, # â­ï¸ live=True ì¶”ê°€ â­ï¸
    allow_flagging="never", # â­ï¸ ë¶ˆí•„ìš”í•œ í”Œë˜ê·¸ ë°©ì§€ â­ï¸
    title="Face2Chat: ê°ì • ì¸ì‹ ìŒì„± ì±—ë´‡",
    description="ì›¹ìº ê³¼ ë§ˆì´í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì •ì„ ì¸ì‹í•˜ê³  ëŒ€í™”í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤."
)

if __name__ == "__main__":
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ë° ì•ˆë‚´
    # Vosk ëª¨ë¸ ê²½ë¡œ í™•ì¸
    vosk_model_path = "models/vosk-model-small-en-us-0.15" # ë˜ëŠ” 'models/vosk-model-ko-0.22'
    if not os.path.exists(vosk_model_path):
        print(f"\n[ê²½ê³ ] Vosk ëª¨ë¸ '{vosk_model_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("Vosk ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ 'models' í´ë”ì— ì••ì¶• í•´ì œí•´ì•¼ í•©ë‹ˆë‹¤.")
        print("ì˜ˆì‹œ: https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip")
        print("ë˜ëŠ” í•œêµ­ì–´ ëª¨ë¸: https://alphacephei.com/vosk/models/vosk-model-ko-0.22.zip")
        print("ë‹¤ìš´ë¡œë“œ í›„ ì••ì¶•ì„ í’€ê³ , ì••ì¶• í•´ì œëœ í´ë” ì´ë¦„ì„ ìœ„ model_pathì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ì„¸ìš”.\n")
        # sys.exit(1) # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì¢…ë£Œí•˜ë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    # YOLOv8 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ (vision_analyzerì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ, ì—¬ê¸°ì„œë„ ì•ˆë‚´ ê°€ëŠ¥)
    # VisionAnalyzer í´ë˜ìŠ¤ ë‚´ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµí•©ë‹ˆë‹¤.

    # Gradio ì•± ì‹¤í–‰
    interface.launch()